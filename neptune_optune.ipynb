{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb0Z8_UdjhYg"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 1000px\" src=\"https://github.com/HSG-AIML-Teaching/GSERM2024-Lab/blob/main/lab_05/banner.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0cP5Z789_rr"
      },
      "source": [
        "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"https://github.com/HSG-AIML-Teaching/GSERM2024-Lab/blob/main/lab_05/hsg_logo.png?raw=1\">\n",
        "\n",
        "##  Lab 05 - Convolutional Neural Networks (CNNs)\n",
        "\n",
        "GSERM Summer School 2024, Deep Learning: Fundamentals and Applications, University of St. Gallen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0S5hxsGjhYi"
      },
      "source": [
        "The lab environment is based on Jupyter Notebooks (https://jupyter.org), which provide an interactive platform for performing a variety of statistical evaluations and data analyses. In this lab, we will learn how to enhance vanilla **Artificial Neural Networks (ANNs)** using `PyTorch` to classify even more complex images. We will explore a special type of deep neural network known as **Convolutional Neural Networks (CNNs)** to achieve this. CNNs leverage the hierarchical pattern in data, allowing them to assemble more complex patterns from smaller, simpler ones. This hierarchical structure enables CNNs to learn a set of discriminative features and subsequently utilize these learned patterns to classify the content of an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rno8GqfC9_rz"
      },
      "source": [
        "The history of CNNs is rich and exhibits pivotal contributions from researchers like *Yann LeCun*, who developed the first practical CNN, known as **LeNet**, in the late 1980s. CNNs have since become a cornerstone in deep learning, significantly advancing the capabilities of image recognition and classification.\n",
        "\n",
        "In this lab, we will use the `PyTorch` library to implement and train a CNN-based neural network. Our network will be trained on tiny images from the **CIFAR-10** dataset, which includes aeroplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. We will utilize the learned CNN model to classify previously unseen images into these distinct categories upon successful training.\n",
        "\n",
        "The figure below illustrates a high-level view of the machine learning process we aim to establish in this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nswYOXvk9_r0"
      },
      "source": [
        "<img align=\"center\" style=\"max-width: 900px\" src=\"https://github.com/HSG-AIML-Teaching/GSERM2024-Lab/blob/main/lab_05/splash.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPRKkkig9_r2"
      },
      "source": [
        "## 2. Setup of the Jupyter Notebook Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mZL4i6W9_r2"
      },
      "source": [
        "Similar to the previous labs, we need to import several Python libraries that facilitate data analysis and visualization. We will primarily use `PyTorch`, `NumPy`, `Scikit-learn`, `Matplotlib`, `Seaborn`, and a few utility libraries throughout this lab:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CZ3L6l3K5FO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9cwWtab9_r2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39412ec4-b793-4352-806f-ba6cd308af5a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m487.9/487.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[neptune] [warning] NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
            "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/luisaellamueller/mlops/e/MLOPS-31\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:03<00:00, 43.0MB/s]\n",
            "[neptune] [warning] NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
            "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
            "        for dictionaries or collections that contain unsupported values.\n",
            "        For more, see https://docs-legacy.neptune.ai/help/value_of_unsupported_type\n",
            "[I 2025-05-12 09:50:11,727] A new study created in memory with name: no-name-baea41c6-c63a-4a6b-aa6e-6ca334fb0f62\n",
            "[I 2025-05-12 09:55:27,783] Trial 0 finished with value: 12.146215677261353 and parameters: {'lr': 0.01, 'batch_size': 32, 'optimizer': 'SGD'}. Best is trial 0 with value: 12.146215677261353.\n",
            "[W 2025-05-12 09:55:28,462] Param lr unique value length is less than 2.\n",
            "[W 2025-05-12 09:55:28,463] Param optimizer unique value length is less than 2.\n",
            "[W 2025-05-12 09:55:28,464] Param batch_size unique value length is less than 2.\n",
            "[W 2025-05-12 09:55:28,465] Param optimizer unique value length is less than 2.\n",
            "[W 2025-05-12 09:55:28,466] Param batch_size unique value length is less than 2.\n",
            "[W 2025-05-12 09:55:28,467] Param lr unique value length is less than 2.\n"
          ]
        }
      ],
      "source": [
        "# =======================\n",
        "# ğŸ§  MLOps CIFAR10 Training - Local Setup (Neptune + Optuna)\n",
        "# =======================\n",
        "\n",
        "# ---- Install Dependencies ----\n",
        "!pip install -q neptune optuna neptune-optuna torchvision torch scikit-learn matplotlib seaborn\n",
        "\n",
        "# ---- Imports ----\n",
        "import os\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import numpy as np\n",
        "import neptune.new as neptune\n",
        "import optuna\n",
        "from neptune.integrations.optuna import NeptuneCallback\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ---- Neptune Init ----\n",
        "run = neptune.init_run(\n",
        "project=\"myproject\",\n",
        "    api_token=\"mytoken\"  # Replace this with your token\n",
        ")\n",
        "\n",
        "# ---- CIFAR10 Dataset ----\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into train, val, test\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "rest_size = len(dataset) - train_size - val_size\n",
        "train_set, val_set, _ = random_split(dataset, [train_size, val_size, rest_size])\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "run[\"data/name\"] = \"CIFAR-10\"\n",
        "run[\"data/source\"] = \"torchvision.datasets\"\n",
        "run[\"data/train_size\"] = len(train_set)\n",
        "run[\"data/val_size\"] = len(val_set)\n",
        "run[\"data/test_size\"] = len(test_dataset)\n",
        "run[\"data/classes\"] = dataset.classes\n",
        "run[\"data/transform\"] = str(transform)\n",
        "\n",
        "# ---- Model Definition ----\n",
        "class CIFAR10Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.logsoftmax(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# ---- Objective Function for Optuna ----\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_categorical(\"lr\", [0.0001, 0.001, 0.005, 0.01])\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"])\n",
        "\n",
        "    # Dataloaders\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=1000)\n",
        "\n",
        "    model = CIFAR10Net()\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr) if optimizer_name == \"SGD\" else optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train\n",
        "    model.train()\n",
        "    for epoch in range(20):\n",
        "        epoch_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        run[\"training/epoch_loss\"].append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    val_acc = correct / len(val_set)\n",
        "    run[\"validation/accuracy\"].append(val_acc)\n",
        "    return val_loss\n",
        "\n",
        "# ---- Optuna Setup ----\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "neptune_callback = NeptuneCallback(run=run)\n",
        "\n",
        "study.optimize(objective, n_trials=24, callbacks=[neptune_callback])\n",
        "\n",
        "# ---- Save Best Model ----\n",
        "best_params = study.best_params\n",
        "run[\"best_hyperparams\"] = best_params\n",
        "\n",
        "# ---- Retrain on full train + val ----\n",
        "combined_set = torch.utils.data.ConcatDataset([train_set, val_set])\n",
        "combined_loader = DataLoader(combined_set, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
        "\n",
        "final_model = CIFAR10Net()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(final_model.parameters(), lr=best_params[\"lr\"]) if best_params[\"optimizer\"] == \"SGD\" else optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
        "\n",
        "final_model.train()\n",
        "for epoch in range(20):\n",
        "    total_loss = 0\n",
        "    for inputs, labels in combined_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = final_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    run[\"final_training/epoch_loss\"].append(total_loss / len(combined_loader))\n",
        "\n",
        "# ---- Save Final Model ----\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "torch.save(final_model.state_dict(), \"models/BEST_CNN.pth\")\n",
        "run[\"model/best_model\"].upload(\"models/BEST_CNN.pth\")\n",
        "\n",
        "# ---- Evaluate on Test ----\n",
        "final_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = final_model(inputs)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.tolist())\n",
        "\n",
        "print(\"Test Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n",
        "\n",
        "run.stop()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "254.39999389648438px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}